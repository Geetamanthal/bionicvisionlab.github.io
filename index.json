[{"authors":["beyeler_michael"],"categories":null,"content":"Michael Beyeler directs the Bionic Vision Lab at UC Santa Barbara.\nHe received a PhD in Computer Science from UC Irvine as well as a BS in Electrical Engineering and a MS in Biomedical Engineering from ETH Zurich, Switzerland. Prior to joining UCSB, he completed a postdoctoral fellowship in the labs of Ione Fine (Psychology, Institute for Neuroengineering) and Ariel Rokem (eScience Institute) at the University of Washington, where he developed computational models of bionic vision. He is Associate Director of the UCSB Center for Virtual Environments and Behavior (ReCVEB) and recipient of the National Institutes of Health (NIH) Pathway to Independence Award.\n","date":1570742503,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1570742503,"objectID":"a30d046c7e5376f816dc02725b57ccf2","permalink":"https://bionicvisionlab.org/authors/beyeler_michael/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/beyeler_michael/","section":"authors","summary":"Michael Beyeler directs the Bionic Vision Lab at UC Santa Barbara.\nHe received a PhD in Computer Science from UC Irvine as well as a BS in Electrical Engineering and a MS in Biomedical Engineering from ETH Zurich, Switzerland. Prior to joining UCSB, he completed a postdoctoral fellowship in the labs of Ione Fine (Psychology, Institute for Neuroengineering) and Ariel Rokem (eScience Institute) at the University of Washington, where he developed computational models of bionic vision.","tags":null,"title":"Michael Beyeler","type":"authors"},{"authors":["kasowski_justin"],"categories":null,"content":"Justin Kasowksi transferred to UC Santa Barbara in 2016 after receiving associates degrees in Biological Sciences and Chemistry from Santa Barbara City College. In 2018, he graduated UCSB with a degree in Molecular, Cellular, and Developmental Biology.\nHe is pursuing his PhD under Dr. Michael Beyeler in the Bionic Vision Lab, where he utilizes virtual reality and computer vision to model the perceptual experience of retinal prosthesis patients.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a48875e7e0a562c5912f6ba595424697","permalink":"https://bionicvisionlab.org/authors/kasowski_justin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kasowski_justin/","section":"authors","summary":"Justin Kasowksi transferred to UC Santa Barbara in 2016 after receiving associates degrees in Biological Sciences and Chemistry from Santa Barbara City College. In 2018, he graduated UCSB with a degree in Molecular, Cellular, and Developmental Biology.\nHe is pursuing his PhD under Dr. Michael Beyeler in the Bionic Vision Lab, where he utilizes virtual reality and computer vision to model the perceptual experience of retinal prosthesis patients.","tags":null,"title":"Justin Kasowski","type":"authors"},{"authors":["raghulan_rashi"],"categories":null,"content":"Rashi Raghulan is an undergraduate student in the Department of Molecular, Cellular, and Development Biology at the University of California, Santa Barbara.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5d2f80621a3c5169ac1de1887b567090","permalink":"https://bionicvisionlab.org/authors/raghulan_rashi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/raghulan_rashi/","section":"authors","summary":"Rashi Raghulan is an undergraduate student in the Department of Molecular, Cellular, and Development Biology at the University of California, Santa Barbara.","tags":null,"title":"Rashi Raghulan","type":"authors"},{"authors":null,"categories":null,"content":" What would the world look like with a bionic eye?\nThis graduate course will introduce students to the multidisciplinary field of bionic vision, with an emphasis on both the computer science and neuroscience of the field.\nThe course will conclude with a programming project (teams of \u0026le; 3, any language/environment ok) in lieu of a final exam, giving students an opportunity to gain hands-on experience of working on open research problems using methods and tools best suited to their scientific background.\n         Instructor Michael Beyeler (first initial last name at ucsb dot edu)   Class WQ 2020, Tue/Thu 9:00 \u0026ndash; 10:50 am, Phelps 3526   Office Hours Tue 4:00 \u0026ndash; 5:00 pm or by appointment, Psych East 3822    This website and the Piazza Forum will be used as centers for communication. Homework submission will occur through GauchoSpace. Make sure you are enrolled! (Come to class to get an add code.)\nTable of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nCourse Objectives The course will give an overview of current bionic eye technology designed to restore vision to people living with incurable blindness. By the end of the course, you should be able to:\n Identify various types of bionic eye technologies, their differences and similarities Have a basic understanding of the neuroscience of the human visual system Be familiar with common preprocessing, encoding, and electrical stimulation methods Understand the limitations of current bionic eye technologies Have hands-on experience of working on open problems in the field  The course is targeted to a diverse audience spanning from computer science (human factors, neural networks, computer vision) to psychology (vision, psychophysics) and brain sciences (computational neuroscience, neuroengineering).\nPrerequisites  There are no official prerequisites for this course. The instructor will do his best to make the course content self-contained. However, prior programming experience (e.g., Python, Matlab, C++) will be highly beneficial as Homework 2 (HW2) and the final project require programming. Students will be introduced to pulse2percept, a Python-based simulation framework for bionic vision, which will form the basis for HW2 and (optionally) the final project.  Schedule Import calendar:  Note: This schedule is subject to change over the course of the quarter.\n  Wk Date Reading Topics Action HW out HW due     1 Tue Jan 7    Introduction: Class requirements, policies Bionic vision: then \u0026amp; now        Thu Jan 9  R1, R2    Blinding eye diseases Sight restoration approaches \u0026amp; challenges     HW1     2 Tue Jan 14  R3    Fundamentals of neuroscience The visual system        Thu Jan 16  R4    Computational neuroscience Introduction to Python   Homework 1 (HW1) due by Sun, Jan 19, 11:59 pm.     A1   HW1   3 Tue Jan 21  R5, R6    Retina in health \u0026amp; disease Retinal prostheses        Thu Jan 23  R7    Introduction to pulse2percept in Python Project ideas: Discussion \u0026amp; brainstorming    A2      4 Tue Jan 28  R8    Visual psychophysics for retinal prostheses Phosphene models: Scoreboard, axon map        Thu Jan 30   Teams present their project ideas  Team \u0026 project description (TPD) due by Sun, Feb 2, 11:59 pm.     HW2 TPD   5 Tue Feb 4  R9    Cortical prostheses: approaches, challenges Phosphene models for cortical prostheses        Thu Feb 6    Activity TBA        6 Tue Feb 11    Optimizing visual outcomes with machine learning        Thu Feb 13   Guest Visit: Jason Esterhuizen, ORION implantee       7 Tue Feb 18    Rehabilitation Perceptual learning        Thu Feb 20    Activity TBA   Homework 2 (HW2) due by Sun, Feb 23, 11:59 pm.      HW2   8 Tue Feb 25    Image preprocessing Peripherals, accessories        Thu Feb 27   Teams present project progress      9 Tue Mar 3  Teams work on projects -- Michael out of the country      Thu Mar 5  Teams work on projects -- Michael out of the country       10 Tue Mar 10   Guest Lecture: Dr. Noelle Stiles, USC/Caltech       Thu Mar 12    Outlook: Future of bionic vision Alternatives to brain-computer interfaces   Project report (PR) \u0026amp; source code (SC) due Sun, Mar 15, 11:59 pm.      PR\u0026amp;SC  11 Tue Mar 17 9a-12p Teams make their final project presentations (date tentative)       Course Requirements \u0026amp; Grading Your final grade will be determined as follows:\n 15% Class participation and attendance:  Students are expected to attend all class sessions and actively participate in class discussions and activities. If a student must miss a session, they should email the instructor beforehand. Each student will be allowed 3 excused absences (no detailed explanation required) before their absence will start to negatively affect their participation grade. However, late arrivals and unexcused absences will most definitely have a negative effect on a student\u0026rsquo;s participation grade.  30% Homework assignments:  10% Homework 1 20% Homework 2  55% Final project implementation, documentation, and presentation  5% Project idea presentation (1 slide) 10% Project progress presentation (2 slides: what have you done, what\u0026rsquo;s left to do) 20% Project final presentation 20% Project final report (+5% extra credit if project shows promise of turning into a publication)   Lateness Policy All assignments are due at 11:59:59 pm on the scheduled due date, typically a Sunday (timestamp of the online submission system).\n Each student will be allowed 3 \u0026ldquo;late days\u0026rdquo; over the course of the quarter for which lateness will not be penalized. Late days cannot be applied to project deadlines. Late days may be applied to one or both homework assignments: Anything turned in between 12:00:00 am and 11:59:59 pm the next day is one day late; every day thereafter that an assignment is late, including weekends and holidays, counts as an additional late day. Absolutely no late work will be accepted after the deadline if you have used up all your late days. If you\u0026rsquo;re not done on time, you must turn in what you have to receive partial credit. There will be no exceptions from this rule. No exceptions will be made for the final project report.  Please make sure you understand this policy.\nFinal Project In lieu of a final exam, students will conduct a programming project (team size \u0026le; 3). The goal of the project is to gain hands-on experience working on open research questions in bionic vision using tools and methods best suited to their scientific background.\nAll projects must address a research question and have a programming component. Students are free to use any programming language and development environment they choose. Building a project based on pulse2percept is encouraged (especially for students with relatively little programming experience) but is by no means required. Reproducing key research findings in the literature is allowed. No pure literature reviews, please.\nProjects that show promise of turning into a publication will receive extra credit.\nStudents will present their project to the rest of the class during finals week. In addition, students will submit a write-up of their project and hand in their source code (see Milestones).\nThe project will be evaluated based on the:\n originality/novelty of the idea technical strength of the work (emphasis on the research, not the programming expertise) organization, clarity, and style of the project report effort and completeness of the work (normalized by the number of team members)  Project Milestones    Date Time Deliverable due     Thu, Jan 23 9:00 am Students start forming teams and discussing project ideas in class.   Thu, Jan 30 9:00 am Teams present their project ideas in class.   Sun, Feb 2 11:59 pm Teams submit a project title and 2-3 sentence project description.   Thu, Feb 27 9:00 am Teams present their project progress in class.   Sun, Mar 12 11:59 pm Teams hand in their final project report and all source code.   Tue, Mar 17 9:00 am (date tentative) Teams make their final project presentations in class.    Students are encouraged to discuss ideas with the instructors, so that feedback can be incorporated early in the process.\nLate days cannot be used on these project deadlines.\nProject Presentation Teams will present their project in class during finals week. Each team will have 20 minutes to present (including start-up time). Every member of the group must speak.\nMore details will be announced soon.\nProject Report Each team will also submit a write-up of their project.\nMore details will be announced soon.\nProject Ideas Apart from the required programming component, students are free to choose a project best suited to their interests and expertise.\nMore details will be announced soon.\nPlease choose a project topic that is related to your interests so that the project is fun for you!\nAcademic Integrity The University of California has formal policies related to academic integrity.\nAny act of academic dishonesty, such as cheating or plagiarism, will result in a University disciplinary action and an \u0026ldquo;F\u0026rdquo; in this course. In addition to academic integrity, I also expect everyone in this class to treat their fellow students and course staff with respect.\nBasic Needs If you are facing any challenges securing food or housing and believe this may affect your performance in the class, you are urged to meet with a Food Security and Calfresh Advocate who is aware of the broad variety of resources that UCSB has to offer (see their drop-in hours at food.ucsb.edu). You are also urged to contact the professor if you are comfortable doing so.\nPlease visit food.ucsb.edu for additional resources including Calfresh, the AS Food Bank, and more.\n","date":1572480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572480000,"objectID":"e575d5ab26bf85a2b9d88aa8cefe5cd2","permalink":"https://bionicvisionlab.org/teaching/2020-winter-cs291i/","publishdate":"2019-10-31T00:00:00Z","relpermalink":"/teaching/2020-winter-cs291i/","section":"teaching","summary":"This graduate course will introduce students to the multidisciplinary field of bionic vision, with an emphasis on both the computer science and neuroscience of the field.","tags":["human-computer interaction","computational neuroscience","visual psychophysics","neuroengineering"],"title":"CS 291I: Bionic Vision","type":"teaching"},{"authors":["Michael Beyeler","Geoffrey M. Boynton","Ione Fine","Ariel Rokem"],"categories":[],"content":"","date":1570742503,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570742503,"objectID":"630c44461f17ba74859ff8fc47a5b6bd","permalink":"https://bionicvisionlab.org/publication/2019-optimal-surgical-placement/","publishdate":"2019-10-10T14:21:43-07:00","relpermalink":"/publication/2019-optimal-surgical-placement/","section":"publication","summary":"We systematically explored the space of possible implant configurations to make recommendations for optimal intraocular positioning of Argus II.","tags":["neuroengineering","retinal implants","computational models","retinitis pigmentosa","retinal implant surgery"],"title":"Model-based recommendations for optimal surgical placement of epiretinal implants","type":"publication"},{"authors":null,"categories":null,"content":"Despite the increasing clinical and commercial use of retinal implants, the perceptual experience of sight recovery (SR) patients is surprisingly poorly understood.\nA common misconception in the field is that each electrode in an array can be thought of as a ‘pixel’ in an image; to generate a complex visual experience, one then simply needs to turn on the right combination of pixels.\nHowever, almost all SR technologies are likely to suffer from perceptual distortions and subsequent loss of information due to interactions between the technology and the underlying neurophysiology.\nThe goal of this project is to characterize these distortions psychophysically, and to develop a computational model that predicts what a patient should see for any given input stimulus.\n","date":1570060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570060800,"objectID":"ba5fa69f72b3505988d005912a425275","permalink":"https://bionicvisionlab.org/research/0-visual-modeling/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/research/0-visual-modeling/","section":"research","summary":"Clinical studies have demonstrated that the vision provided by current SR devices differs substantially from normal sight.","tags":["computational neuroscience","visual psychophysics","computational models"],"title":"What would the world look like with a bionic eye?","type":"research"},{"authors":null,"categories":null,"content":"Our lack of understanding of multi-electrode interactions severely limits current stimulation protocols. For example, current Argus II protocols simply attempt to minimize electric field interactions by maximizing phase delays across electrodes using ‘time-multiplexing’. The assumption is that single-electrode percepts act as atomic ‘building blocks’ of patterned vision. However, these building blocks often fail to assemble into more complex percepts.\nThe goal of this project is therefore to develop new stimulation strategies that minimize perceptual distortions.\n","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569974400,"objectID":"63a49ce091f31ff634ace90a3b7c5471","permalink":"https://bionicvisionlab.org/research/1-optimize-stimulation/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/research/1-optimize-stimulation/","section":"research","summary":"Rather than predicting perceptual distortions, one needs to solve the inverse problem: What is the best stimulus to generate a desired visual percept?","tags":["computer vision","machine learning","neuroengineering","data science"],"title":"How can we design more effective stimulation strategies?","type":"research"},{"authors":null,"categories":null,"content":"Embedding a computational model that can predict the perceptual distortions encountered by sight restoration (SR) patients in virtual reality (VR) will enable sighted subjects to act as virtual patients in real-world tasks. This will allow us to test novel stimulation strategies in high-throughput experiments. Strategies that result in good VR performance will then be validated in real SR patients.\nFor example, rather than aiming to ‘restore natural vision’, there is potential merit in borrowing computer vision algorithms as preprocessing techniques to maximize the usefulness of prosthetic vision. Edge enhancement and contrast maximization are already routinely used in Argus II. In the future, more sophisticated techniques such as low-level image enhancements and visual saliency based transforms could further improve visual performance.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"5d07c563c9daf78e1945a87f9ab55a11","permalink":"https://bionicvisionlab.org/research/2-virtual-patients/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/research/2-virtual-patients/","section":"research","summary":"Novel stimulation strategies can be tested on sighted subjects viewing a simulation of prosthetic vision in virtual/augmented reality.","tags":["virtual reality","augmented reality","visual psychophysics"],"title":"How can virtual patients improve performance of real sight recovery patients?","type":"research"},{"authors":["Bingni W. Brunton","Michael Beyeler"],"categories":null,"content":"","date":1563321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563321600,"objectID":"43b69a7df75a2dada5a23a4fe3f8f960","permalink":"https://bionicvisionlab.org/publication/2019-data-driven-models-human-neuroscience-neuroengineering/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-data-driven-models-human-neuroscience-neuroengineering/","section":"publication","summary":"In this review, we provide an accessible primer to modern modeling approaches and highlight recent data-driven discoveries in the domains of neuroimaging, single-neuron and neuronal population responses, and device neuroengineering.","tags":["neuroscience","neuroengineering","data science","computational models"],"title":"Data-driven models in human neuroscience and neuroengineering","type":"publication"},{"authors":["Michael Beyeler"],"categories":[],"content":"A new article appeared in PCMag to celebrate the inauguration of the Bionic Vision Lab at UCSB:\n Bionic vision might sound like science fiction, but Dr. Michael Beyeler is working on just that.\nOriginally from Switzerland, Dr. Beyeler is wrapping up his postdoctoral fellow at the University of Washington before moving to the University of California Santa Barbara this fall to head up the newly formed Bionic Vision Lab in the Departments of Computer Science and Psychological \u0026amp; Brain Sciences.\nWe spoke with him about this \u0026ldquo;deep fascination with the brain\u0026rdquo; and how he hopes his work will eventually be able to restore vision to the blind. Here are edited and condensed excerpts from our conversation.\n Read the full article here.\n","date":1562689740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562689753,"objectID":"00c60f07f684f1b3efbeb9fa775c1b01","permalink":"https://bionicvisionlab.org/post/2019-07-pcmag/","publishdate":"2019-07-09T09:29:00-07:00","relpermalink":"/post/2019-07-pcmag/","section":"post","summary":"Michael Beyeler recently sat down with PCMag to talk about bionic vision and his move to UC Santa Barbara.","tags":[],"title":"PCMag: Restoring vision with bionic eyes - no longer science fiction","type":"post"},{"authors":["Michael Beyeler","Emily L. Rounds","Kristofor D. Carlson","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1561593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561593600,"objectID":"bbcd0bf9d8bf5b4421dda0e383b60bd0","permalink":"https://bionicvisionlab.org/publication/2019-neural-correlates-sparse-coding-dimensionality-reduction/","publishdate":"2019-06-27T00:00:00Z","relpermalink":"/publication/2019-neural-correlates-sparse-coding-dimensionality-reduction/","section":"publication","summary":"Brains face the fundamental challenge of extracting relevant information from high-dimensional external stimuli in order to form the neural basis that can guide an organism's behavior and its interaction with the world. One potential approach to addressing this challenge is to reduce the number of variables required to represent a particular input space (i.e., dimensionality reduction). We review compelling evidence that a range of neuronal responses can be understood as an emergent property of nonnegative sparse coding (NSC)—a form of efficient population coding due to dimensionality reduction and sparsity constraints.","tags":["computational models","sparse coding","population coding","dimensionality reduction","nonnegative matrix factorization"],"title":"Neural correlates of sparse coding and dimensionality reduction","type":"publication"},{"authors":["Michael Beyeler","Devyani Nanduri","James D. Weiland","Ariel Rokem","Geoffrey M. Boynton","Ione Fine"],"categories":null,"content":"","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"2dd7c8662626103dafead1ce078e67a2","permalink":"https://bionicvisionlab.org/publication/2019-axon-map-model/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/2019-axon-map-model/","section":"publication","summary":"We show that the perceptual experience of retinal implant users can be accurately predicted using a computational model that simulates each individual patient’s retinal ganglion axon pathways.","tags":["neuroengineering","retinal implants","computational models","retinitis pigmentosa","biomedical engineering","pattern vision","perception"],"title":"A model of ganglion axon pathways accounts for percepts elicited by retinal implants","type":"publication"},{"authors":["Michael Beyeler"],"categories":null,"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"590205e420483d3eb975cab6dcf29af1","permalink":"https://bionicvisionlab.org/publication/2019-biophysical-model-axonal-stimulation/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/publication/2019-biophysical-model-axonal-stimulation/","section":"publication","summary":"To investigate the effect of axonal stimulation on the retinal response, we developed a computational model of a small population of morphologically and biophysically detailed retinal ganglion cells, and simulated their response to epiretinal electrical stimulation. We found that activation thresholds of ganglion cell somas and axons varied systematically with both stimulus pulse duration and electrode-retina distance. These findings have important implications for the improvement of stimulus encoding methods for epiretinal prostheses.","tags":["neuroengineering","retinal implants","computational models","retinitis pigmentosa","biomedical engineering","spiking neural networks"],"title":"Biophysical model of axonal stimulation in epiretinal visual prostheses","type":"publication"},{"authors":["Michael Beyeler"],"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"bdb5d890074ce8a33df96462da08c953","permalink":"https://bionicvisionlab.org/publication/2019-retinal-sheet-transplants/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/2019-retinal-sheet-transplants/","section":"publication","summary":"A Commentary on: Detailed Visual Cortical Responses Generated by Retinal Sheet Transplants in Rats with Severe Retinal Degeneration by Foik, A. T., Lean, G. A., Scholl, L. R., McLelland, B. T., Mathur, A., Aramant, R. B., et al. (2018). J. Neurosci. 38, 10709–10724. doi: 10.1523/JNEUROSCI.1279-18.2018","tags":["neuroengineering","retinal sheet transplants"],"title":"Commentary: Detailed visual cortical responses generated by retinal sheet transplants in rats with severe retinal degeneration","type":"publication"},{"authors":["Ting-Shou Chou","Hirak J. Kashyap","Jinwei Xing","Stanislav Listopad","Emily L. Rounds","Michael Beyeler","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1531440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531440000,"objectID":"4139e2680a25a36a0f14e370e72e1352","permalink":"https://bionicvisionlab.org/publication/2018-carlsim4/","publishdate":"2018-07-13T00:00:00Z","relpermalink":"/publication/2018-carlsim4/","section":"publication","summary":"We have developed CARLsim 4, a user-friendly SNN library written in C++ that can simulate large biologically detailed neural networks. Improving on the efficiency and scalability of earlier releases, the present release allows for the simulation using multiple GPUs and multiple CPU cores concurrently in a heterogeneous computing cluster. Benchmarking results demonstrate simulation of 8.6 million neurons and 0.48 billion synapses using 4 GPUs and up to 60x speedup for multi-GPU implementations over a single-threaded CPU implementation, making CARLsim 4 well-suited for large-scale SNN models in the presence of real-time constraints.","tags":["computational models","spiking neural networks","CARLsim","GPU","parallel programming","C++"],"title":"CARLsim 4: An open source library for large scale, biologically detailed spiking neural network simulation using heterogeneous clusters","type":"publication"},{"authors":["Michael Beyeler","Ariel Rokem","Geoffrey M. Boynton","Ione Fine"],"categories":null,"content":"","date":1503360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503360000,"objectID":"82c721904459e434fcac59d3e13ec296","permalink":"https://bionicvisionlab.org/publication/2017-learning-to-see-again/","publishdate":"2017-08-22T00:00:00Z","relpermalink":"/publication/2017-learning-to-see-again/","section":"publication","summary":"The goal of this review is to summarize the vast basic science literature on developmental and adult cortical plasticity with an emphasis on how this literature might relate to the field of prosthetic vision.","tags":["neuroengineering","retinal implants","cortical plasticity","perceptual learning","retinitis pigmentosa","macular degeneration","biomedical engineering"],"title":"Learning to see again: Biological constraints on cortical plasticity and the implications for sight restoration technologies","type":"publication"},{"authors":["Michael Beyeler","Geoffrey M. Boynton","Ione Fine","Ariel Rokem"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"2d6cce377cc9ed496868935f2dbb3ea4","permalink":"https://bionicvisionlab.org/publication/2017-pulse2percept/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/2017-pulse2percept/","section":"publication","summary":"*pulse2percept* is an open-source Python simulation framework used to predict the perceptual experience of retinal prosthesis patients across a wide range of implant configurations.","tags":["neuroengineering","retinal implants","open-source","data science","computational models","retinitis pigmentosa","biomedical engineering","Python","parallel programming"],"title":"pulse2percept: A Python-based simulation framework for bionic vision","type":"publication"},{"authors":["Michael Beyeler","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"7bde0b15775f3061c93cf9d26d8a722b","permalink":"https://bionicvisionlab.org/publication/2016-sparse-decomposition-model/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/2016-sparse-decomposition-model/","section":"publication","summary":"Using a dimensionality reduction technique known as non-negative matrix factorization, we found that a variety of medial superior temporal (MSTd) neural response properties could be derived from MT-like input features. The responses that emerge from this technique, such as 3D translation and rotation selectivity, spiral tuning, and heading selectivity, can account for a number of empirical results. These findings (1) provide a further step toward a scientific understanding of the often nonintuitive response properties of MSTd neurons; (2) suggest that response properties, such as complex motion tuning and heading selectivity, might simply be a byproduct of MSTd neurons performing dimensionality reduction on their inputs; and (3) imply that motion perception in the cortex is consistent with ideas from the efficient-coding and free-energy principles.","tags":["computational models","visual motion perception","MSTd","optic flow","nonnegative matrix factorization"],"title":"3D visual response properties of MSTd emerge from an efficient, sparse population code","type":"publication"},{"authors":["Michael Beyeler","Nicolas Oros","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"fa66b3464f53a69c81e83355d550df5d","permalink":"https://bionicvisionlab.org/publication/2015-gpu-visually-guided-robot-navigation/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/2015-gpu-visually-guided-robot-navigation/","section":"publication","summary":"We present a cortical neural network model for visually guided navigation that has been embodied on a physical robot exploring a real-world environment. The model includes a rate based motion energy model for area V1, and a spiking neural network model for cortical area MT. The model generates a cortical representation of optic flow, determines the position of objects based on motion discontinuities, and combines these signals with the representation of a goal location to produce motor commands that successfully steer the robot around obstacles toward the goal. This study demonstrates how neural signals in a model of cortical area MT might provide sufficient motion information to steer a physical robot on human-like paths around obstacles in a real-world environment.","tags":["computational models","spiking neural networks","CARLsim","GPU","parallel programming","visual motion perception","pattern motion selectivity","MT","visually guided navigation","robot navigation"],"title":"A GPU-accelerated cortical neural network model for visually guided robot navigation","type":"publication"},{"authors":["Michael Beyeler","Kristofor D. Carlson","Ting-Shou Chou","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1436659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436659200,"objectID":"924b641054ea0e69270600c5c06b1975","permalink":"https://bionicvisionlab.org/publication/2015-carlsim3/","publishdate":"2015-07-12T00:00:00Z","relpermalink":"/publication/2015-carlsim3/","section":"publication","summary":"We have developed CARLsim 3, a user-friendly, GPU-accelerated SNN library written in C/C++ that is capable of simulating biologically detailed neural models. The present release of CARLsim provides a number of improvements over our prior SNN library to allow the user to easily analyze simulation data, explore synaptic plasticity rules, and automate parameter tuning. In the present paper, we provide examples and performance benchmarks highlighting the library's features.","tags":["computational models","spiking neural networks","CARLsim","GPU","parallel programming","C++"],"title":"CARLsim 3: A user-friendly and highly optimized library for the creation of neurobiologically detailed spiking neural networks","type":"publication"},{"authors":["Michael Beyeler","Florian Mirus","Alexander Verl"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"f64e29214acef64cd605d0dee5f7df1e","permalink":"https://bionicvisionlab.org/publication/2014-vision-road-lane-detection/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/2014-vision-road-lane-detection/","section":"publication","summary":"This paper presents an integrative approach to ego-lane detection that aims to be as simple as possible to enable real-time computation while being able to adapt to a variety of urban and rural traffic scenarios. The approach at hand combines and extends a road segmentation method in an illumination-invariant color image, lane markings detection using a ridge operator, and road geometry estimation using RANdom SAmple Consensus (RANSAC). The power and robustness of this algorithm has been demonstrated in a car simulation system as well as in the challenging KITTI data base of real-world urban traffic scenarios.","tags":["computational models","computer vision","driver information systems","intelligent transportation systems","image segmentation","object detection","road traffic","KITTI"],"title":"Vision-based robust road lane detection in urban environments","type":"publication"},{"authors":["Michael Beyeler","Micah Richert","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1391558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391558400,"objectID":"96b515fcb79c860869313bf9febe2c91","permalink":"https://bionicvisionlab.org/publication/2014-snn-pattern-motion/","publishdate":"2014-01-20T00:00:00Z","relpermalink":"/publication/2014-snn-pattern-motion/","section":"publication","summary":"We present a two-stage model of visual area MT that we believe to be the first large-scale spiking network to demonstrate pattern direction selectivity. In this model, component-direction-selective (CDS) cells in MT linearly combine inputs from V1 cells that have spatiotemporal receptive fields according to the motion energy model of Simoncelli and Heeger. Pattern-direction-selective (PDS) cells in MT are constructed by pooling over MT CDS cells with a wide range of preferred directions. Responses of our model neurons are comparable to electrophysiological results for grating and plaid stimuli as well as speed tuning.","tags":["spiking neural networks","visual motion perception","pattern motion selectivity","MT","parallel programming","GPU","CARLsim"],"title":"Efficient spiking neural network model of pattern motion selectivity in visual cortex","type":"publication"},{"authors":["Kristofor D. Carlson","Michael Beyeler","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1390176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390176000,"objectID":"ae0ad4a16dae311c5e5d007164a39eee","permalink":"https://bionicvisionlab.org/publication/2014-gpgpu-accelerated-simulation-parameter-tuning/","publishdate":"2014-01-20T00:00:00Z","relpermalink":"/publication/2014-gpgpu-accelerated-simulation-parameter-tuning/","section":"publication","summary":"We describe a simulation environment that can be used to design, construct, and run spiking neural networks (SNNs) quickly and efficiently using graphics processing units (GPUs). We then explain how the design of the simulation environment utilizes the parallel processing power of GPUs to simulate large-scale SNNs and describe recent modeling experiments performed using the simulator. Finally, we present an automated parameter tuning framework that utilizes the simulation environment and evolutionary algorithms to tune SNNs.","tags":["neuromorphic engineering","spiking neural networks","evolutionary algorithms","parallel programming","GPU"],"title":"GPGPU accelerated simulation and parameter tuning for neuromorphic applications","type":"publication"},{"authors":["Michael Beyeler","Nikil Dutt","Jeffrey L. Krichmar"],"categories":null,"content":"","date":1385856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385856000,"objectID":"4b3672d3c8372629471a2a3248359d59","permalink":"https://bionicvisionlab.org/publication/2013-categorization-mnist-stdp/","publishdate":"2013-12-01T00:00:00Z","relpermalink":"/publication/2013-categorization-mnist-stdp/","section":"publication","summary":"We present a large-scale model of a hierarchical spiking neural network (SNN) that integrates a low-level memory encoding mechanism with a higher-level decision process to perform a visual classification task in real-time. The model consists of Izhikevich neurons and conductance-based synapses for realistic approximation of neuronal dynamics, a spike-timing-dependent plasticity (STDP) synaptic learning rule with additional synaptic dynamics for memory encoding, and an accumulator model for memory retrieval and categorization. The full network, which comprised 71,026 neurons and approximately 133 million synapses, ran in real-time on a single off-the-shelf graphics processing unit (GPU). The network achieved 92% correct classifications on MNIST in 100 rounds of random sub-sampling, which is comparable to other SNN approaches and provides a conservative and reliable performance metric. Additionally, the model correctly predicted reaction times from psychophysical experiments. Because of the scalability of the approach and its neurobiological fidelity, the current model can be extended to an efficient neuromorphic implementation that supports more generalized object recognition and decision-making architectures found in the brain.","tags":["computational models","spiking neural networks","synaptic plasticity","parallel programming","GPU","MNIST"],"title":"Categorization and decision-making in a neurobiologically plausible spiking network using a STDP-like plasticity rule","type":"publication"},{"authors":["Michael Beyeler","Fabio Stefanini","Henning Proske","Giovanni Galizia","Elisabetta Chicca"],"categories":null,"content":"","date":1288569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288569600,"objectID":"bc1e436831b5c9690ee42ff2b36e90a3","permalink":"https://bionicvisionlab.org/publication/2010-exploring-olfactory-networks/","publishdate":"2010-11-01T00:00:00Z","relpermalink":"/publication/2010-exploring-olfactory-networks/","section":"publication","summary":"Olfactory stimuli are represented in a high-dimensional space by neural networks of the olfactory system. While a number of studies have illustrated the importance of inhibitory networks within the olfactory bulb or the antennal lobe for the shaping and processing of olfactory information, it is not clear how exactly these inhibitory networks are organized to provide filtering and contrast enhancement capabilities. In this work the aim is to study the topology of the proposed networks by using software simulations and hardware implementation. While we can study the dependence of the activity on each parameter of the theoretical models with the simulations, it is important to understand whether the models can be used in robotic applications for real-time odor recognition. We present the results of a linear simulation, a spiking simulation with I\u0026F neurons and a real-time hardware emulation using neuromorphic VLSI chips.","tags":["neuromorphic engineering","olfaction"],"title":"Exploring olfactory sensory networks: Simulations and hardware emulation","type":"publication"}]